\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{adjustbox}
\usepackage{inputenc}
\usepackage{caption}
\usepackage[top=1.2in, bottom=1.2in, left=1.2in, right=1.2in]{geometry}
\usepackage{bbm}
\usepackage{bigints}

\usepackage{float}
\setlength\parindent{0pt}

\usepackage{calc}
\usepackage[super]{nth}

\usepackage{accents}
\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}


\usepackage{setspace}
%\singlespacing
\onehalfspacing
%\doublespacing

\title{Weighted Splitting in Random Forests}
\author{Christian Hilscher}


\begin{document}
\maketitle

\section{Introduction}

\include{DTRF}



\section{Concepts}

\subsection{MDI}

\include{pardepfunc}

\subsection{End-cut-property}

Central to the CART algorithm and thus decision trees and random forests is the splitting rule. The default way of determining split points is by maximising the decrease in impurity as described in section XYZ. \\
When looking for the optimal split point on a noisy variable, the CART algorithm has the tendency to split at the extreme of the feature space. \textit{End-cut-property} or \textit{end-cut-preference} (ECP) was first mentioned and documented by Breiman et al. (1984). When a variable is noisy, it does not have an effect on $Y$ and therefore the decrease in impurity is asymptotically zero everywhere. When looking for a split point in practise however, the largest decrease in impurity can often be found at the edges by cutting off the first or last couple of observations. The reasoning behind this is that the more observations in either node, the closer will the sample variance of that child-node become to the variance in the parent node and thus lowering the decrease in impurity. Theorem 11.1 in Breiman et al. (1984) book formally describes this mechanism. \\


\subsection{Drawbacks of ECP}
The literature up to now regarded ECP mostly as a negative effect of the CART algorithm. Splitting at the edges of the feature space produces two very unbalanced nodes: a tiny amount of observations in one node and the rest in the other. The main problem with this behavior is that the observations in the small node do not constitute a good local predictor. Even in an asymptotic setting where $n \rightarrow \infty$, it is not guaranteed, that the number of observations in \textit{every} node also approaches infinity. Since splitting on the edge leaves only a couple of observations in one of the resulting child-nodes, the number of observations in that node can stay fairly small and the predictor then does not converge to its true value. \\
Researchers have found several ways to mitigate this problem. The most straight forward approach is the one also adopted in Athey and Wager (2018). They impose a fraction $\alpha$ which is the minimum fraction of leaves that need to be in any child-node. The resulting trees are then called $\alpha$-regular and as $n$ grows, so does the number of observations in each node. Scornet (2015) imposes that at least $k$ observations have to be in each node and this way forces prevents nodes at the top of the tree from becoming too small. Denil, Matheson and De Freitas (2014) employ slightly different procedure. For a dimension $j$ they randomly draw five observations and then only consider those points between the smallest and the largest of the five observations as possible split points. Compared to the first approach, here it does not seem as if the number of observations in each node goes to infinity. The restrictions however force the CART algorithm to search for the largest decrease of impurity in the middle and both papers show that this leads to the overall number of observations in every node to increase as $n$ grows. \\
All of these approaches have the same goal: making sure that splits do not happen at the extremes of the feature space while keeping as much from the original CART algorithm as possible. From a theoretical point of view, ECP is seen as a problem because it hinders the analysis in an asymptotic setting. In applied work it is mainly unwanted because it leads to bad predictors for those observations which end up in those small nodes. All major software packages have therefore implementations of the approaches described above and the default values of all of them are such that splits at the edges are discouraged.

\subsection{Potential Benefits}
The verdict on end-cut-preference is not unanimous however, as Ishwaran (2014) highlights multiple benefits of the end-cut-preference exhibited by the CART algorithm. The main argument is built upon the local adaptivity of random forests. The ECP leads the child-nodes to be very imbalanced in size with the majority of the sample ending up in one child-node just a few observations in the other one. The advantage now comes from the preservation of most of the sample. Since the split on the noisy variable is not informative, only a few observations are "lost" while almost all other observations are used in splits further down the tree. This way the decision tree can recover from a bad split along a noisy variable. Ishwaran also mentions that ECP can occur for strong variables in regions where the parent-node is situated in a region of the feature space where the signal is relatively low. Especially in random forests where for each node only a subset of variables is considered as splitting dimension, the possibility of considering only variables with low signal increases, making the ability to recover from bad splits even more important. In simulation studies as well as with real world datasets, Ishwaran finds that algorithms possessing the ECP are in almost all setups superior to the ones without end-cut-preference. \\
Another fairly trivial situation where ECP is beneficial is when one faces outliers in the data. In such a case cutting at the edges is naturally what one would like the algorithm to do. A priori it is however unclear whether outliers in the data constitute genuine outliers or whether they happen to have large error terms. Ideally one has an algorithm which can pick up these differences and adapt accordingly.

\section{Weighted Splitting}
As early as in Breiman et al. (1984) a weighted splitting rule was proposed to mitigate the ECP of the standard CART procedure. The idea is to modify the splitting rule such that it takes the location of the split points into account and penalizes splits towards the edges of the feature space. Klusowski (2019) mentions that one could adapt the splitting rule and instead maximize 

\begin{align}
    \Delta_{\alpha}(s; \mathbf{t}) &= [4 P(\mathbf{t}_L) P(\mathbf{t}_R)]^{\alpha} \ \Delta (s; \mathbf{t}) \label{delta_alpha} \\
    &= \omega \ \Delta (s; \mathbf{t}) \nonumber
\end{align}

where $\Delta (s; \mathbf{t})$ is the decrease in impurity which is maximized in the standard CART algorithm. $P(\mathbf{t}_L)$ and $P(\mathbf{t}_R)$ are the probabilities that an observation ends up in the left and right node and $\omega \leq 1$. These quantities naturally depend on the location of the split point. In case of a split in at the median of the observations, they are equal and the weight is maximized at $\omega = 1$. The more imbalanced the nodes become, the smaller the weight will be. The parameter $\alpha$ acts as a regularizer which governs how severely imbalances and thus split points at the edges are penalized. \\
Interestingly, (\ref{delta_alpha}) reveals a trade-off between the decrease in impurity and the node balancedness. When maximizing $\Delta_{\alpha}$, a split near the edges is likelier if the associated decrease in impurity is large. For small decreases in impurity on the other hand, the weighted splitting favors splits more in the middle.


\section{Theoretical Results}

Most of the theoretical results build upon the framework laid out by Klusowski (2019) since its results are the most general ones up to now. Adding the weight into the maximization problem makes some alterations to his proofs necessary but the overall framework is still valid.

\subsection{Importance of MDI}



\newpage
\nocite{*}
\begin{singlespace}
  \bibliographystyle{acm}
\bibliography{annot}
\end{singlespace}

\end{document}

