\par{

\section{Concepts}
The purpose of this section is to lay out two concepts on which the theoretical analysis will build upon. Since the consistency proof of the adapted algorithm is done by extending Klusowski's (2019) work, the notation closely follows his. Two concepts, which allow us to get an insight into how big the impact of one variable $X_j$ on the outcome is, are laid out in what follows.


\subsection{Partial Dependence Function}

It is useful to have a coherent method of measuring the importance a specific variable $X_j$ has on the output $Y$. 
Since partial dependence plots usually only depict the relationship between two variables, we follow Klusowski (2019) by introducing a conditional partial dependence function. 
Recall that we assume $Y_i = m(\mathbf{X}_i) + \epsilon_i$. 
The idea behind the conditional partial dependence function is to look at the influence of one specific $X_j$ while ignoring the others. 
For this let 

\begin{align}
    \bar{F}_j(x_j, \mathbf{t}) = E [Y \ | \ \mathbf{X} \in \mathbf{t}, X_j = x_j] \label{pardepfunc_f}
\end{align}

where $\bar{F}_j(x_j, \mathbf{t})$ is the partial dependence function for variable $j$ at point $x_j$ conditional on being in node $\mathbf{t}$. 
One can also think of (\ref{pardepfunc_f}}) as a solution to a least squares approximation of $m(\mathbf{X})$ as a function of only $X_j$ within node $\mathbf{t}$.
For later purposes it will be beneficial to additionally define the mean centered partial dependence function 

\begin{align}
    \bar{G}_j(x_j, \mathbf{t}) &= \bar{F}_j(x_j, \mathbf{t}) - E [Y \ | \ \mathbf{X} \in \mathbf{t}] \nonumber \\
    &= E [Y \ | \ \mathbf{X} \in \mathbf{t}, X_j = x_j] - E [Y \ | \ \mathbf{X} \in \mathbf{t}] \label{pardepfunc_g}
\end{align}

which is the partial dependence function demeaned by the average of all observation within the respective node. \\

This concept now allows us to reformulate our definition of strong and weak variables.
Since weak variables $\{X_j : j \notin \mathcal{S}\}$ are independent of $Y$, by law of iterated expectations, (\ref{pardepfunc_f}) is equal to $E [Y \ | \ \mathbf{X} \in \mathbf{t}]$.
This in turn implies that the demeaned partial dependence function $\bar{G}_j(x_j, \mathbf{t})$ is zero for all weak variables.
Strong variables in contrast, have an effect on $Y$ and thus the demeaned partial dependence function is not necessarily 0. 
Note however, that here we concern ourselves with the population parameters. 
In a finite sample analysis, it is indeed the case that because of the error term, we can have $\bar{G}_j(x_j, \mathbf{t}) \neq 0$ even for weak variables. 
This will in fact play an important part in the later analysis when constructing a decision tree. 

\subsection{MDI - Mean Decrease in Impurity}

Another measure for ranking variables which has found broad appeal in the practical application of random forests is the mean decrease in impurity (MDI). 
The intuitive idea behind it is to determine the influence of each variable on the output. 
Each split in a random forest is made to decrease to variance of a node by dividing it into two, more homogenous child-nodes. 
We called the associated decrease in variance $\hat{\Delta}(j, \hat{s}, \mathbf{t})$ which happens when splitting the node $\mathbf{t}$ on variable $j$ with split point $s$. \\
The MDI for a variable $j$ then is defined as the average of all decreases in variance when splitting on that variable, averaged over all trees in a random forest.

\begin{align}
    \widehat{MDI}(X_j) &= \frac{1}{N_T} \sum_T \widehat{MDI}(X_j, T) \nonumber \\
    \widehat{MDI}(X_j, T) &= \sum_{\substack{\mathbf{t'} \\ j = j_{\mathbf{t'}}}} \frac{n_{\mathbf{t'}}}{n} \hat{\Delta}(j, \hat{s}, \mathbf{t'}) \label{MDI_nogood}
\end{align}

For calculating the MDI of a variable of a specific tree $T$, the sum is taken over all non-terminal nodes $\mathbf{t'}$. 
The associated decrease in impurity when split on $j$ is weighted by the number of observations in the respective node relative to the total number of observations. \\

Klusowski (2019) shows that (\ref{MDI_nogood}) can be re-written to 

\begin{align}
    \widehat{MDI}(X_j, \mathbf{t}) &= \sum_{\substack{\mathbf{t'} \supset \mathbf{t} \\ j = j_{\mathbf{t'}}}} \hat{w}(j, \hat{s}, \mathbf{t'}) \hat{\Delta}(j, \hat{s}, \mathbf{t'}) \label{MDI}
\end{align}

where $\mathbf{t}$ now is a terminal node and $\mathbf{t'}$ are all ancestor nodes of $\mathbf{t}$. 
The sum is taken over all splits in which variable $X_j$ was selected as splitting dimension and $\hat{w}(j, \hat{s}, \mathbf{t'})$ are non-negative weights. \\
Analyzing the infinite sample properties of the weights, Klusowski is able to express the weights as a function of the demeaned partial dependence function and the decrease in impurity

\begin{align}
    w(j, s^*, \mathbf{t'}) = \frac{1}{|\bar{G}_j(s^*, \mathbf{t'})|^2 + \Delta(j, s^*, \mathbf{t'})} \label{weight}
\end{align}

where the empirical parameters $\hat{\Delta}$ and $\hat{s}$ are replaced by their population counterparts.

\subsection{MDI and Bias}

Building on work by Louppe et al. (2013) for extremely randomized forests, Xi et al. (2019) highlight the connection between the bias of a random forest and the MDI for the CART algorithm. 
More specifically, they show that the bias is upper-bounded by the sum of the MDI of all weak variables. 
From (\ref{weight}) we know that the MDI is a function of the decrease in impurity $\hat{\Delta}(s, \mathbf{t})$ and the demeaned partial dependence function $\bar{G}_j(s^*, \mathbf{t})$.
In expectation both terms are zero and the MDI for weak variables is 0 as well since the random forest never splits on the weak dimensions. 
Considering a finite sample setting however, it can very well be the case that both terms are different from zero, and we thus have a positive MDI for weak variables.
One intuitive explanation for the results of Xi et al. is that in theory the random forest should never split on noisy variables since they are independent of $Y$. 
Because of finite sample properties it can happen however, that the algorithm finds the largest decrease in impurity on a noisy variable. 
The more often this happens, the larger the sum of the MDI for noisy variables and the higher the bias of that forest. \\
While Xi et al. (2019) offer insights on the relationship between weak variables and the bias, Klusowski (2019) goes the opposite route: he is able to characterize the bias as a function of the MDI of the strong variables. 
With that, he is then in the position to prove consistency of the random forest for the most general class of functions yet. To get a clearer understanding, the two main theorems of his work are repeated here. \\

The first result sheds light on the length of a variable $j$ in node $\mathbf{t}$ as a function of the MDI. 
Under some relatively mild assumptions on the predictor variables, it can be shown that the node length in direction $j$ is negatively proportional to the MDI of that variable.

\begin{theorem}
    Assume $[a_j(\mathbf{t}),\ b_j(\mathbf{t})]$ is subnode in the $j^{th}$ direction for the terminal node $\mathbf{t} = \Pi_{j=1}^d [a_j(\mathbf{t}),\ b_j(\mathbf{t})]$. Then,

    \begin{align}
        \mathbbm{P}_{\mathbf{X}} [a_j(\mathbf{t}) \leq X_j \leq b_j(\mathbf{t})] \leq \text{exp} \left\{- \frac{\eta}{4} MDI(X_j, \mathbf{t}) \right\} \label{theorem1}
    \end{align}
\end{theorem}

with $\eta$ being a constant and $MDI(X_j, \mathbf{t})$ the infinite sample counterpart of (\ref{MDI}).
This result says that the length of the terminal node in the $j^{th}$ direction is smaller, the higher the MDI of that variable for that particular node is.
There are two takeaways which resemble the ones from the findings of Lin and Jeon (2006), who analyze the random forest through the lens of an adaptive nearest neighbor estimator. 
The first is that the side lengths of important variables are smaller, hence the regression tree splits more often on strong variables.
Second, the regression tree is able to exploit local changes and thus depends on the particular terminal node $\mathbf{t}$. 
Even for the same variable, the side lengths can differ, with regions exhibiting a strong signal being split more often. \\

The most important implication of Theorem 1 is that it allows us to make the bridge to the conditions for the consistency of partitioning estimators as proposed by Stone (1977). 
One necessary condition for consistency is that the diameter of every terminal node converges to 0 for every strong variable. 
This is the case in our setup if $MDI(X_j, \mathbf{t}) \rightarrow \infty \ \forall j \in S$. \\
Klusowski proceeds by showing that one can lower bound $MDI(X_j, \mathbf{t})$ by the selection frequency of $X_j$ and a measure of balancedness. 
Defining

\begin{align}
    \lambda_j(\mathbf{t}) = 4 P_j(\mathbf{t}_L^*) P_j(\mathbf{t}_R^*) = 1 - |P_j(\mathbf{t}_L^*) - P_j(\mathbf{t}_R^*)|^2 \nonumber
\end{align}

as a measure of node balancedness, it is in indicator for the amount of data in either child node. 
$P_j(\mathbf{t}_L^*)$ is the relative amount of observations which would fall into the left child node in an infinite sample setting when splitting node $\mathbf{t}$ on variable $j$.
If we were split at the median, $\lambda_j(\mathbf{t})$ would be 1, the further to either edge we go, the lower will $\lambda_j(\mathbf{t})$ be.
To globalize this measure, we can take the infimum over all parent nodes and arrive at 

\begin{align}
    \Lambda_j = \inf_{\mathbf{t}} \lambda_j(\mathbf{t}) \label{lambda}
\end{align}

Being able to assign to each variable a measure of global balancedness $\Lambda_j$, we are in a position to state Klusowski's second theorem.

\begin{theorem}
    Assume that 
    \begin{align}
        \quad \sup_{x_j \in [0,1]} \inf_{r \geq 1} \{ r: \frac{\partial^r}{\partial x^r_j} f(x_j, \mathbf{x}_{\backslash j}) \text{ is nonzero and continuous for all } \mathbf{x}_{\backslash j} \in [0, 1]^{d-1} \} \text{ and} \label{assumption_theorem2}
    \end{align}

    is finite and $\Lambda_j > 0 \ \forall j \in \mathcal{S}$. In addition, assume that all variables of $\mathbf{X}$ are independent and have marginal densities which are continuous and never vanish. Then, 
    \begin{align}
        MDI(X_j, \mathbf{t}) \geq \Lambda_j K_j(\mathbf{t}) \label{theorem2}
    \end{align}

    where $K_j$ is the number of times variable $X_j$ was selected as splitting dimension across all parent nodes of the terminal node $\mathbf{t}$.
\end{theorem}

Theorem 2 together with Theorem 1 imply that the diameter of each terminal node converges to zero in the $j^{th}$ as $K_j(\mathbf{t}) \rightarrow \infty$ for all strong variables. Drawing on the insight of Lin and Jeon (2006), we know that more important variables are chosen more often as splitting dimension. As $n \rightarrow \infty$, the selection frequency of those strong variables increases with tree size. 

