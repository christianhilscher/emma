\par{
\section{Decision Trees and Random Forest}

Unfortunately there is no one clear algorithm on how to build binary decision trees and random forests. Rather each paper has its own procedures and while mostly similar, they differ in some important aspects. For this reason we specify the exact tree building process and further aggregation into random forests. 

\subsection{Preliminaries}

We start with our training data $\mathcal{D} = \{(\mathbf{X}_1, Y_1), ..., (\mathbf{X}_n, Y_n)\}$ where $\mathbf{X}_i \in [0, 1]^d$ and $Y_i \in \mathbbm{R}$ is a continuous response variable for $1 \leq i \leq n$. The $j^{\text{th}}$ coordinate of the input matrix $\mathbf{X}$ is denoted by $X_j$. We assume $Y_i = m(\mathbf{X}_i) + \epsilon_i$ with $m(\mathbf{x}) = \mathbbm{E}[Y | \mathbf{X} = \mathbf{x}]$ being an unknown regression function and $\epsilon_i$ is an $i.i.d$ error. The main goal is to estimate $m(\mathbf{x})$ and make predictions $\hat{Y}(\mathbf{x})$. The accuracy of the predictions will be determined by the mean squared error $\mathbbm{E}[(\hat{Y}(\mathbf{X}) - m(\mathbf{X}))^2]$ which we seek to minimize. \\
Random forests are often used in setups with a large amount of input variables of which only a few actually determine the outcome variable $Y$. Therefore, we assume that $m(\mathbf{x}) = \mathbbm{E}[Y | \mathbf{X} = \mathbf{x}]$ depends only on a small subset $\mathcal{S} < d$ features, which we also call strong features. On the other hand, all weak (noisy) variables $\{X_j: j \notin \mathcal{S} \}$ are independent of $Y$.

\subsection{Decision Tree}
A decision tree is a collection of recursive binary splits which aim to partition the data into more and more homogeneous subgroups. 
In other words, we apply the same process to smaller and smaller data until we arrive at a certain stopping criterion. \\
At each step, one variable and one split point are chosen which determine how the data will be partitioned. Consider the case where we arrive at node $\mathbf{t}$, splitting on variable $X_j$ and $s$ being the split point. The data will be separated into the two child nodes $\mathbf{t}_L = \{ \mathbf{X} \in \mathbf{t}: X_j \leq s \}$ and $\mathbf{t}_R = \{ \mathbf{X} \in \mathbf{t}: X_j > s \}$. This procedure is repeated until a specified stopping criterion. \\

The question arising now is how to choose the splitting dimension and the split point. Breiman et al. (1984) propose the CART (Classification and Regression Tree) algorithm which is the common way to construct decision trees. The ultimate goal is to partition the data into homogeneous subgroups, measured by the within-node variance. The CART algorithm therefore aims to reduce the variance of the response variable $Y$ within a node by choosing the best splitting dimension and location. \\
The variance of a node $\mathbf{t}$ is given by

\begin{align}
    \hat{\Delta}(\mathbf{t}) = \frac{1}{N(\mathbf{t})} \sum_{\mathbf{X}_i \in \mathbf{t}} (Y_i - \hat{Y}_\mathbf{t})^2 \label{node_variance}
\end{align}

where $\hat{Y}_\mathbf{t}$ is the sample mean of the responses and $N(\mathbf{t})$ is the number of observations in node $\mathbf{t}$ respectively. To assess the quality of different split points as dimensions, we compare them using a measure called \textit{decrease in impurity}. The decrease in impurity for a generic variable $X_j$ and split point $s$ is given by

\begin{align}
    \hat{\Delta}(s; \mathbf{t}) = \hat{\Delta}(\mathbf{t}) - [\hat{P}(\mathbf{t}_L) \hat{\Delta}(\mathbf{t}_L) + \hat{P}(\mathbf{t}_R) \hat{\Delta}(\mathbf{t}_R)] \label{Delta}
\end{align}

with $\hat{P}(\mathbf{t}_L) = \frac{N(\mathbf{t}_L)}{N(\mathbf{t})}$ and $\hat{P}(\mathbf{t}_R) = \frac{N(\mathbf{t}_R)}{N(\mathbf{t})}$ being the fractions of observations falling into the left and right child node. Maximizing (\ref{Delta}) with respect to $s$ then yields the split point which in turn partitions the data such that the resulting child nodes are as homogeneous as possible. Iterating over all input variables contained in $\mathbf{X}$ then allows to choose the splitting dimension $X_j$ and respective $s$ with the largest impurity reduction. \\
The estimator for a terminal node $\mathbf{t}$ is given by the sample mean of all observations falling into that node: $\hat{Y} = \bar{Y}_{\mathbf{t}}$. \\

While single decision trees have been empirically shown to have rather low biases, they do exhibit high variance. This behavior is especially pronounced when the stopping criterion is set in such way, that each terminal node contains only one observation. In other words, if the tree is fully grown, each terminal node is made up of one observation. Splitting up to that point makes the tree very unstable and when facing different errors the tree would look markedly different. Thus, while being a really local predictor, the high variance of the estimator is one drawback of this approach. One possibility to mitigate this problem is to make the tree stop via a pre-specified stopping criterion. Imposing that each terminal node has to contain at least $k > 1$ observations, leads to multiple observations in the terminal nodes and reduces the variance of the tree. With an increasing $k$, the tree becomes more and more shallow. Naturally, one faces another trade-off with this imposition: The higher $k$, the more observations in a terminal node and this makes the estimator less local. Especially with small $k$ however, the reduction in variance more than makes up for a little bit larger bias such that in practice single decision trees are not grown fully.


\subsection{Random Forest}

Another remedy to reduce the variance of a decision tree while trying to keep the favorable property of low bias was introduced in Breiman (2001)

\subsection{Connection to kNN-Estimator}
Lin and Jeon (2006) show that a random forest is adaptive nearest neighbor estimator. 
The neighborhood of a point $x_0$ for example are all points which are in the same terminal node as $x_0$.
The adaptive component comes from the fact that the number of neighbors can differ depending on the neighborhood. This feature allows the random forest to adapt to the regression function and exploit local changes.
Looking at a decision tree through the lens of a kNN estimator gives us the benefit that we can study certain properties in a more familiar setting.
Especially when trying to understand on which variables the decision tree splits the data, the perspective from the kNN estimator proves insightful.
Lin and Jeon show that a decision tree splits more often on variables which are "important", meaning that they explain a large share of the variance of the outcome variable. 
To intuitively understand their argument, assume the following simple setup 

\begin{align*}
    Y &= g(\mathbf{X}) + \epsilon \\
    g(\mathbf{X}) &= \sum_{j \in \mathcal{S}} a_j X_j
\end{align*}

Here, $Y$ is a simple additive model of our strong variables and $\epsilon$ and i.i.d error term.
Assuming further that all $X_j \sim \text{U}[0, 1]$, the importance of a particular $X_j$ on $Y$ is given by $|a_j|$. Variables with a large $|a_j|$ influence the outcome variable $Y$ more than those with a low value. \\


Consider the case that we want to estimate $\hat{Y}_0$ for a given $\mathbf{X}_0$ for the actual outcome $Y_0$. 
Then the goal is to characterize the neighborhood aiming to best approximate $g(\mathbf{X})$ around $\mathbf{X}_0$. 
This is done by choosing intervals for each $X_j$ such that the area within all those intervals consists of $n$ points and whose average constitutes our estimate $\hat{Y}_0$. 
Let's call the interval lengths for each variable $q_j$. Lin and Jeon (2006) show that in equilibrium, $q_j |a_j| = C \ \forall j$ for any constant $C$. 
In other words, in the optimal case, all variables have the same importance. 

Turning to our example with $X_j \sim \text{U}[0, 1]$, this implies that variables with a high $|a_j|$ have shorter intervals $q_j$. 
Intuitively, this means that for variables which have a large effect on $Y$, the interval around $\mathbf{X_0}$ should be rather small since being far away from $\mathbf{X_0}$ would push our estimate too far away from the actual outcome $Y_0$. 
The small interval makes it thus less likely to underfit the data.
On the other hand, for variables with a low importance, we can allow for a wider interval, mitigating the possibility of overfitting the data. 
The optimality condition from Lin and Jeon (2006) means that more important variables have shorter interval lengths in optimum. \\

We can now take these insights and apply them to our random forest setup.
The neighborhood around $\mathbf{X}_0$ are now all observations which are in the same terminal node as $\mathbf{X}_0$. 
The side lengths $q_j$ correspond to the interval of $X_j$ which defines the terminal node. 
One interesting aspect of this example is that the optimal $q_j$ can vary from node to node, showcasing the local adaptivity of the random forest estimator.

The other, for this exposition more relevant insight however, is that variables which have a high importance have smaller side lengths. 
To get smaller side lengths, the algorithm must split more often on those particular variables. 
Put differently, strong variables will be chosen more often to split on in a random forest than weak  variables. 
This ties into the findings of Klusowski (2019), who finds a negative relationship between the selection frequency of $X_j$ and the respective importance of variable $j$ measured by its MDI (mean decrease in impurity). \\

The simple additive model above can explain intuitively why strong variables are more often chosen as splitting dimension.
Klusowski's finding holds for a far more general class of regression functions, making it clear that the relative contribution of the variable $j$ to the total variance of $Y$ plays an important role for the number of times it is selected as splitting dimension.



\subsection{Literature Overview}

The theoretical properties of random forests have been studied quite intensively in the last couple of years. Decision trees and methods building on them are among the most widely used algorithms in the fields of supervised machine learning. Despite their broad usage, the exact theoretical characteristics have proved to be difficult to analyze. \\
It is especially the data-dependency of the splitting points which complicates the analysis of random forest estimators. For this reason, most of the literature has made adaptions to the original CART algorithm and to the way tree within a forest are constructed. \\
Conditions for consistency of data-independent estimators have been proposed by Stone (1977). Decision trees being partitioning estimators, the first attempts to characterize the theoretical properties aimed to describe the behavior of data-independent algorithms. Cutler and Zhao (2001) achieve this by first randomly choosing the splitting variable and afterwards randomly selecting the split point. These alterations make the estimator independent of the data insofar as that the probability of a split happening on a particular variable for example is a not influenced by the underlying data. Another way to abstract from the data was analyzed by Breiman (2004) where the splitting dimension is chosen randomly and the split point is always the median observation. This way the structure of the tree is again data-independent and consistency can be shown relatively easily. \\
Biau (2012) was the first to show consistency results for an algorithm which more closely resembles the original random forest procedure and is data-dependent. The split points are determined by their respective decrease in impurity. In Biau's setup, the split points do not maximize the empirical decrease in impurity but rather the asymptotic one. For the consistent estimation of these asymptotic values a second dataset is needed however, since using the same data for estimation and locating the split points would lead to inconsistencies. Building on these results, Scornet et al. (2015) are able to prove consistency for a data-dependent random forest, albeit only for additive models. The restriction of allowing only an additive structure can be traced back to the influential paper by Lin and Jeon (2006) who reformulate the random forest as an adaptive nearest neighbor estimator. This way they can show that the variable selection frequency, that is the number of times a specific variable was selected to split on, is related to its "importance". The more of variation in $Y$ in a specific neighborhood $X_j$ can explain, the more frequently will the split happen on variable $j$. In an additive model the influence of one variable only depends on that variable itself since there are not interactions terms, which in turn render the analysis of the process more amenable. \\
Klusowski (2019) adds to the literature by proving consistency for a more general class of response surfaces which go beyond the additive structure used in Scornet et al. (2015). Overall the study of theoretical properties of the random forest algorithm has proved challenging and all the adaptions to the original algorithm prevents simple comparisons of approaches.
}